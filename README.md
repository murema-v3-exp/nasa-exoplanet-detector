# NASA Exoplanet Hunter ğŸŒŒğŸ”­# NASA Exoplanet Hunter ğŸŒŒğŸ”­# NASA Exoplanet Hunte---

[![NASA Space Apps 2025](https://img.shields.io/badge/NASA%20Space%20Apps-2025-blue)](https://www.spaceappschallenge.org/)

[![Python 3.12+](https://img.shields.io/badge/python-3.12+-blue.svg)](https://www.python.org/downloads/)

[![FastAPI](https://img.shields.io/badge/FastAPI-0.104+-green.svg)](https://fastapi.tiangolo.com/)[![NASA Space Apps 2025](https://img.shields.io/badge/NASA%20Space%20Apps-2025-blue)](https://www.spaceappschallenge.org/)##
ğŸš€ Quick Start

**AI-powered exoplanet detection system** built by **Murema Manganyi**,
**Thando**, and **Hlali** for NASA Space
Apps 2025.[![Python 3.12+](https://img.shields.io/badge/python-3.12+-blue.svg)](https://www.python.org/downloads/)

ğŸ¯ **Goal:** Help astronomers identify promising exoplanet candidates from
Kepler/K2/TESS mission data using machine
learning.[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)###
Prerequisites

**Status:** âœ… **MVP Complete** - Working Streamlit app + FastAPI backend with
87% recall- Python 3.12+

---### NASA Space Apps Challenge 2025- pip

## ğŸš€ Quick Start- (Optional) Node.js 18+ for React frontend

### Installation**AI-powered exoplanet detection system** built by **Murema Manganyi**, **Thando**, and **Hlali**.

````bash

# Clone repository### Installation

git clone https://github.com/murema-v3-exp/nasa-exoplanet-detector.git

cd nasa-exoplanet-detectorğŸ¯ **Goal:** Help astronomers prioritize follow-up observations by identifying the most promising exoplanet candidates from Kepler/K2/TESS mission data using AI and machine learning.```bash



# Create virtual environment# Clone the repository

python -m venv venv

venv\Scripts\activate  # Windows**Current Status:** âœ… **MVP Complete** - Working Streamlit app with 87% recall, now preparing production React + FastAPI architecture.git clone https://github.com/YOUR_USERNAME/nasa-exoplanet-detector.git



# Install dependenciescd nasa-exoplanet-detector

pip install -r requirements.txt

```---



### Run Streamlit App# Create virtual environment

```bash

streamlit run app_enhanced.py## ğŸš€ Quick Startpython -m venv venv

````

ğŸŒ Open http://localhost:8501source venv/bin/activate # On Windows:
venv\Scripts\activate

### Run FastAPI Backend### Prerequisites

```bash

uvicorn backend.main:app --reload --port 8000- Python 3.12+# Install dependencies

```

ğŸ“š API Docs: http://localhost:8000/docs- pippip install -r requirements.txt

### Test API- (Optional) Node.js 18+ for React frontend```

```bash

python test_api.py

```

### Installation### Run Streamlit MVP (Current)

---

`bash`bash

## âœ¨ Features

# Clone the repositorystreamlit run app_enhanced.py

### Streamlit Web App âœ…

-   ğŸ“¤ CSV upload (drag-and-drop)git clone
    https://github.com/YOUR_USERNAME/nasa-exoplanet-detector.git```

-   ğŸ¯ XGBoost predictions with 87% recall

-   ğŸšï¸ Interactive threshold slidercd nasa-exoplanet-detectorï¿½ Open
    http://localhost:8501 in your browser

-   ğŸ“Š Probability histogram

-   ğŸ“ˆ Feature importance chart

-   ğŸ’¾ Clean CSV exports (9 columns)

-   ğŸ” Filter by confidence level# Create virtual environment### Run FastAPI
    Backend (In Development)

### FastAPI Backend âœ…python -m venv venv```bash

-   ğŸ”Œ 6 REST API endpoints

-   ğŸ“¦ Model registry (auto-load models)venv\Scripts\activate # Windows# Start
    API server

-   ğŸš€ Fast predictions (~2s for 9K samples)

-   ğŸ“Š Performance metrics APIcd backend

-   ğŸ¯ Feature importance API

-   â¤ï¸ Health check endpoint# Install dependenciespython main.py

-   ğŸ“– Interactive docs (Swagger UI)

pip install -r requirements.txt# Or

---

```uvicorn backend.main:app --reload --port 8000

## ğŸ“Š Model Performance

```

| Metric | Score |

|--------|-------|### Run Streamlit MVPğŸ“š API Docs: http://localhost:8000/docs

| **Recall** | 87.1% |

| **Precision** | 81.8% |```bash

| **ROC-AUC** | 89.4% |

| **F1 Score** | 84.3% |streamlit run app_enhanced.py---

**Cross-Validation (5-Fold):** 85.67% Â± 1.87% recall```

**Dataset:** 9,201 Kepler samples (50/50 split)ğŸŒ Open http://localhost:8501 in
your browser## âœ¨ Features

---

## ğŸ—‚ï¸ Project Structure---### Current (Streamlit MVP)

````- âœ… **CSV Upload**: Drag-and-drop Kepler/K2/TESS format data

nasa-exoplanet-detector/

â”œâ”€â”€ app_enhanced.py            # Streamlit web app## âœ¨ Features- âœ… **Instant Predictions**: XGBoost model with 87% recall

â”œâ”€â”€ test_api.py               # API test suite

â”œâ”€â”€ requirements.txt          # Dependencies- âœ… **Interactive Threshold**: Adjust confidence slider (0.0-1.0)

â”‚

â”œâ”€â”€ src/                      # Core ML pipeline### Current (Streamlit MVP) âœ…- âœ… **Visualizations**:

â”‚   â”œâ”€â”€ preprocessing.py     # Data loading/cleaning

â”‚   â”œâ”€â”€ features.py          # Feature extraction- **CSV Upload**: Drag-and-drop Kepler/K2/TESS format data  - Probability histogram with threshold line

â”‚   â””â”€â”€ scaling.py           # Feature scaling

â”‚- **Instant Predictions**: XGBoost model with 87.1% recall  - Feature importance chart

â”œâ”€â”€ backend/                  # FastAPI REST API

â”‚   â”œâ”€â”€ main.py              # FastAPI app- **Interactive Threshold**: Adjust confidence slider (0.0-1.0)- âœ… **Export Options**:

â”‚   â”œâ”€â”€ api/                 # Endpoints

â”‚   â”œâ”€â”€ core/                # Config & registry- **Visualizations**: Probability histogram + feature importance chart  - Clean CSV (9 essential columns)

â”‚   â””â”€â”€ schemas/             # Pydantic models

â”‚- **Export Options**: Clean CSV (9 columns) or Full CSV (82 columns)  - Full CSV (all 82 columns)

â”œâ”€â”€ scripts/                  # Training scripts

â”‚   â”œâ”€â”€ train_xgb.py         # Train model- **Filtering**: View all/planets/false positives/high confidence- âœ… **Filtering**: View all/planets/false positives/high confidence

â”‚   â”œâ”€â”€ cross_validate.py    # CV evaluation

â”‚   â”œâ”€â”€ data_diagnostic.py   # Data analysis- **Performance**: Processes 9,000+ samples in ~2 seconds- âœ… **Performance**: Processes 9,000+ samples in ~2 seconds

â”‚   â””â”€â”€ model_diagnostic.py  # Model analysis

â”‚

â”œâ”€â”€ data/                     # NASA datasets

â”‚   â”œâ”€â”€ Keppler.csv          # 9,564 samples### Coming Soon (React + FastAPI) ğŸ”„### Coming Soon (React + FastAPI)

â”‚   â”œâ”€â”€ K2.csv

â”‚   â””â”€â”€ TESS.csv- Multi-model support (XGBoost, CNN, Ensemble)- ğŸ”„ Multi-model support (XGBoost, CNN, Ensemble)

â”‚

â””â”€â”€ models/                   # Trained artifacts- Real-time collaboration- ğŸ”„ Real-time collaboration

    â”œâ”€â”€ xgb.pkl              # XGBoost model

    â”œâ”€â”€ scaler.pkl           # Scaler- Batch processing API- ğŸ”„ Batch processing API

    â””â”€â”€ cv_*.csv             # CV results

```- Cloud deployment (AWS/GCP/Azure)- ğŸ”„ Cloud deployment (AWS/GCP/Azure)



---- Advanced visualizations (D3.js)- ğŸ”„ Advanced visualizations (D3.js)



## ğŸ”§ Usage- ğŸ”„ User authentication



### Train Model---

```bash

python scripts/train_xgb.py---

````

## ğŸ“Š Model Performance

### Run Cross-Validation

````bash## ğŸ“Š Model Performance

python scripts/cross_validate.py

```| Metric | Score | Target |



### Data Diagnostics|--------|-------|--------|| Metric | Score | Target |

```bash

python scripts/data_diagnostic.py| **Recall** | 87.1% | >80% âœ… ||--------|-------|--------|

````

| **Precision** | 81.8% | - || **Recall** | 87.1% | >80% âœ… |

### Model Diagnostics

````bash| **ROC-AUC** | 89.4% | - || **Precision** | 81.8% | - |

python scripts/model_diagnostic.py

```| **F1 Score** | 84.3% | - || **ROC-AUC** | 89.4% | - |



---| **F1 Score** | 84.3% | - |



## ğŸ“š API Endpoints**Cross-Validation (5-Fold Stratified):**



### Health Check- Mean Recall: 85.67% Â± 1.87%**Cross-Validation (5-Fold Stratified):**

```bash

GET /api/health- All folds >83%- Mean Recall: 85.67% Â± 1.87%

````

-   ROC-AUC: 89.37% Â± 0.41%- All folds >83%

### List Models

````bash- ROC-AUC: 89.37% Â± 0.41%

GET /api/models

```**Training Data:** 9,201 Kepler samples (4,610 planets, 4,591 false positives)



### Model Metrics**Training Data:** 9,201 Kepler samples (4,610 planets, 4,591 false positives)

```bash

GET /api/models/xgb/metrics---

````

---

### Feature Importance

````bash## ğŸ—‚ï¸ Project Structure

GET /api/models/xgb/importance

```## ğŸ—‚ï¸ Project Structure



### Predict```

```bash

POST /api/predictnasa-exoplanet-detector/```

  - file: CSV file

  - model: xgb (default)â”œâ”€â”€ app_enhanced.py            # Enhanced Streamlit app (recommended)nasa-exoplanet-detector/

  - threshold: 0.5 (default)

```â”œâ”€â”€ requirements.txt           # Python dependenciesâ”œâ”€â”€ app.py                      # Original Streamlit app



**See:** `API_IMPLEMENTATION.md` for detailed docsâ”œâ”€â”€ PROJECT_SPEC.md           # Full technical specificationâ”œâ”€â”€ app_enhanced.py            # Enhanced Streamlit app (recommended)



---â”œâ”€â”€ API_IMPLEMENTATION.md     # Backend implementation guideâ”œâ”€â”€ requirements.txt           # Python dependencies



## ğŸŒŸ Tech Stackâ”‚â”œâ”€â”€ PROJECT_SPEC.md           # Full technical specification



**ML:** XGBoost, scikit-learn, pandas, numpy  â”œâ”€â”€ data/                     # Raw astronomical dataâ”œâ”€â”€ API_IMPLEMENTATION.md     # Backend implementation guide

**Backend:** FastAPI, Uvicorn, Pydantic

**Frontend:** Streamlit, Plotly  â”‚   â”œâ”€â”€ Keppler.csv          # Kepler mission (9,564 samples)â”‚

**Data:** NASA Kepler/K2/TESS missions

â”‚   â”œâ”€â”€ K2.csv               # K2 missionâ”œâ”€â”€ data/                     # Raw astronomical data

---

â”‚   â””â”€â”€ TESS.csv             # TESS missionâ”‚   â”œâ”€â”€ Keppler.csv          # Kepler mission (9,564 samples)

## ğŸ“ˆ Roadmap

â”‚â”‚   â”œâ”€â”€ K2.csv               # K2 mission

- [x] XGBoost baseline (87% recall)

- [x] Streamlit MVPâ”œâ”€â”€ models/                   # Trained model artifactsâ”‚   â””â”€â”€ TESS.csv             # TESS mission

- [x] FastAPI backend

- [x] Interactive API docsâ”‚   â”œâ”€â”€ xgb.pkl              # XGBoost modelâ”‚

- [ ] React frontend (in progress - teammate)

- [ ] CNN time-series modelâ”‚   â”œâ”€â”€ scaler.pkl           # Feature scalerâ”œâ”€â”€ models/                   # Trained model artifacts

- [ ] Ensemble model

- [ ] Cloud deploymentâ”‚   â”œâ”€â”€ cv_fold_metrics.csv  # Cross-validation resultsâ”‚   â”œâ”€â”€ xgb.pkl              # XGBoost model



---â”‚   â””â”€â”€ cv_predictions.csv   # Validation predictionsâ”‚   â”œâ”€â”€ scaler.pkl           # Feature scaler



## ğŸ“„ Documentationâ”‚â”‚   â”œâ”€â”€ cv_fold_metrics.csv  # Cross-validation results



- **[PROJECT_SPEC.md](PROJECT_SPEC.md)** - Full technical specificationâ”œâ”€â”€ src/                      # Core ML pipelineâ”‚   â””â”€â”€ cv_predictions.csv   # Validation predictions

- **[API_IMPLEMENTATION.md](API_IMPLEMENTATION.md)** - Backend implementation guide

- **[task.txt](task.txt)** - Original NASA challengeâ”‚   â”œâ”€â”€ preprocessing.py     # Data loading and cleaningâ”‚



---â”‚   â”œâ”€â”€ features.py          # Feature extractionâ”œâ”€â”€ src/                      # Core ML pipeline



## ğŸ¤ Teamâ”‚   â”œâ”€â”€ scaling.py           # Feature scaling utilitiesâ”‚   â”œâ”€â”€ preprocessing.py     # Data loading and cleaning



- **Murema Manganyi** - ML/Backendâ”‚   â”œâ”€â”€ train_xgb.py         # Training scriptâ”‚   â”œâ”€â”€ features.py          # Feature extraction

- **Thando** - Frontend

- **Hlali** - Team memberâ”‚   â”œâ”€â”€ cross_validate.py    # 5-fold CV scriptâ”‚   â”œâ”€â”€ scaling.py           # Feature scaling utilities



---â”‚   â”œâ”€â”€ data_diagnostic.py   # Data quality analysisâ”‚   â”œâ”€â”€ ml_model.py          # XGBoost training



## ğŸ“§ Contactâ”‚   â””â”€â”€ model_diagnostic.py  # Model behavior analysisâ”‚   â”œâ”€â”€ cnn_model.py         # CNN architecture (future)



**Repository:** https://github.com/murema-v3-exp/nasa-exoplanet-detector  â”‚â”‚   â”œâ”€â”€ ensemble.py          # Ensemble logic (future)

**Challenge:** NASA Space Apps 2025

â””â”€â”€ backend/                  # FastAPI application (in development)â”‚   â”œâ”€â”€ train_xgb.py         # Training script

---

    â””â”€â”€ (see API_IMPLEMENTATION.md)â”‚   â”œâ”€â”€ cross_validate.py    # 5-fold CV script

<div align="center">

```â”‚   â”œâ”€â”€ data_diagnostic.py   # Data quality analysis

**Built with â¤ï¸ for NASA Space Apps 2025**

â”‚   â””â”€â”€ model_diagnostic.py  # Model behavior analysis

*Finding new worlds, one prediction at a time.* ğŸŒâœ¨

---â”‚

</div>

â”œâ”€â”€ notebooks/                # Jupyter notebooks

## ğŸ”§ Usage Examplesâ”‚   â””â”€â”€ (exploratory analysis)

â”‚

### Streamlit Appâ”œâ”€â”€ backend/                  # FastAPI application (in development)

```bashâ”‚   â”œâ”€â”€ main.py              # FastAPI app

streamlit run app_enhanced.pyâ”‚   â”œâ”€â”€ core/

# Upload CSV in browser â†’ Adjust threshold â†’ Download resultsâ”‚   â”‚   â”œâ”€â”€ config.py        # Configuration

```â”‚   â”‚   â””â”€â”€ model_registry.py # Model loading

â”‚   â”œâ”€â”€ api/

### Python API (Direct)â”‚   â”‚   â”œâ”€â”€ predict.py       # Prediction endpoints

```pythonâ”‚   â”‚   â”œâ”€â”€ models.py        # Model info endpoints

from src.preprocessing import load_csv, clean_dataâ”‚   â”‚   â””â”€â”€ health.py        # Health check

from src.features import extract_featuresâ”‚   â””â”€â”€ schemas/

from src.scaling import FeatureScalerâ”‚       â”œâ”€â”€ prediction.py    # Request/response schemas

import joblibâ”‚       â””â”€â”€ response.py      # Additional schemas

â”‚

# Load dataâ””â”€â”€ frontend/                 # React application (teammate's work)

df = load_csv('data/Keppler.csv')    â””â”€â”€ (coming soon)

df_clean = clean_data(df)```

df_features = extract_features(df_clean, mode='catalog')

---

# Load model

model = joblib.load('models/xgb.pkl')## ï¿½ Usage Examples

scaler = FeatureScaler.load('models/scaler.pkl')

### Streamlit App

# Predict```bash

X = df_features.drop('label', axis=1)# Start app

X_scaled = scaler.transform(X)streamlit run app_enhanced.py

probabilities = model.predict_proba(X_scaled)[:, 1]

predictions = (probabilities >= 0.5).astype(int)# Upload CSV in browser

```# Adjust threshold slider

# View predictions and charts

---# Download results

````

## ğŸ“š Documentation

### API (When Backend is Ready)

-   **[PROJECT_SPEC.md](PROJECT_SPEC.md)** - Complete technical specification
    (frontend + backend)```bash

-   **[API_IMPLEMENTATION.md](API_IMPLEMENTATION.md)** - Step-by-step backend
    guide# Predict exoplanets

-   **[task.txt](task.txt)** - Original NASA challenge requirementscurl -X POST
    http://localhost:8000/api/predict \

    -F "file=@data/Keppler.csv" \

--- -F "model=xgb" \

-F "threshold=0.5"

## ğŸ“‚ Datasets & Research

# Get model metrics

### NASA Data Sourcescurl http://localhost:8000/api/models/xgb/metrics

-   **[Kepler Objects of Interest (KOI)](https://exoplanetarchive.ipac.caltech.edu/docs/API_kepobjects.html)** -
    Confirmed planets, candidates, and false positives

-   **[TESS Objects of Interest (TOI)](https://tess.mit.edu/science/toi/)** -
    TESS mission data# Feature importance

-   **[K2 Planets and Candidates](https://exoplanetarchive.ipac.caltech.edu/docs/K2_objects.html)** -
    K2 mission datacurl http://localhost:8000/api/models/xgb/importance

````

### Research Articles

- **[Exoplanet Detection Using Machine Learning](https://arxiv.org/abs/2007.14348)** - Survey of ML approaches### Python API (Direct)

- **[Ensemble-Based ML for Exoplanet ID](https://arxiv.org/abs/2102.06730)** - Preprocessing and ensemble methods```python

from src.preprocessing import load_csv, clean_data

---from src.features import extract_features

from src.scaling import FeatureScaler

## ğŸ§ª Developmentimport joblib



### Training Models# Load data

```bashdf = load_csv('data/Keppler.csv')

# Cross-validationdf_clean = clean_data(df)

python src/cross_validate.pydf_features = extract_features(df_clean, mode='catalog')



# Train and save model# Load model

python src/train_xgb.pymodel = joblib.load('models/xgb.pkl')

scaler = FeatureScaler.load('models/scaler.pkl')

# Run diagnostics

python src/model_diagnostic.py# Predict

```X = df_features.drop('label', axis=1)

X_scaled = scaler.transform(X)

---probabilities = model.predict_proba(X_scaled)[:, 1]

predictions = (probabilities >= 0.5).astype(int)

## ğŸŒŸ Key Technologies```[NASA Space Apps 2025](https://img.shields.io/badge/NASA%20Space%20Apps-2025-blue)](https://www.spaceappschallenge.org/)

[![Python 3.12+](https://img.shields.io/badge/python-3.12+-blue.svg)](https://www.python.org/downloads/)

**Machine Learning:** XGBoost, scikit-learn, pandas, numpy  [![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

**Current Stack:** Streamlit, Plotly

**Target Stack:** React 18 + TypeScript, FastAPI, Vite, TanStack Query, Recharts/D3.js### NASA Space Apps Challenge 2025



---**AI-powered exoplanet detection system** built by **Murema Manganyi**, **Thando**, and **Hlali**.



## ğŸ“ˆ RoadmapğŸ¯ **Goal:** Help astronomers prioritize follow-up observations by identifying the most promising exoplanet candidates from Kepler/K2/TESS mission data using AI and machine learning.



### Phase 1: MVP âœ… (Complete)**Current Status:** âœ… **MVP Complete** - Working Streamlit app with 87% recall, now preparing production React + FastAPI architecture. Away: Hunting for Exoplanets with AI ğŸŒŒ

- [x] XGBoost baseline model (87% recall)

- [x] Feature engineering pipeline### NASA Space Apps Challenge 2025

- [x] Streamlit UI with threshold slider

- [x] Cross-validation (85.67% Â± 1.87%)This project was built for the 2025 NASA Space Apps Challenge by **Murema Manganyi**, **Thando**, and **Hlali**.

- [x] Clean CSV exports

Our goal is to explore how artificial intelligence and machine learning can be used to **automatically identify exoplanets** from space mission data â€” something that has mostly been done manually in the past.

### Phase 2: Backend ğŸ”„ (In Progress)

- [ ] FastAPI REST APIWeâ€™ll be using open datasets from **NASAâ€™s Kepler, K2, and TESS missions**, and focusing on building a model that can classify data points as **confirmed exoplanets, planetary candidates, or false positives**.

- [ ] Model registry

- [ ] Batch processing---



### Phase 3: Advanced ML ğŸ”œ## ğŸ§  Project Overview

- [ ] 1D CNN for time-series

- [ ] Ensemble model (XGB + CNN)The challenge asks us to train a machine learning model on NASAâ€™s open exoplanet datasets and design a **web interface** that makes it easier for scientists or enthusiasts to interact with the data.



### Phase 4: Production ğŸ”œOur plan:

- [ ] React frontend

- [ ] Cloud deployment- Start with the **Kepler Objects of Interest (KOI)** dataset for initial model training.

- [ ] CI/CD pipeline- Experiment with supervised learning methods to classify new data points.

- Integrate a simple, interactive web interface that allows users to input new data and get predictions.

---- Evaluate accuracy and see how preprocessing choices affect model performance.



## ğŸ¤ Contributing---



1. Fork the repository## ğŸ“‚ Datasets & Resources

2. Create a feature branch (`git checkout -b feature/amazing-feature`)

3. Commit your changes (`git commit -m 'Add amazing feature'`)Below are the main resources weâ€™re using to guide and inform our work.

4. Push to the branch (`git push origin feature/amazing-feature`)

5. Open a Pull Request### NASA Data & Resources



---- **Kepler Objects of Interest (KOI)**

  Comprehensive list of confirmed exoplanets, planetary candidates, and false positives from the Kepler mission.

## ğŸ“ License  The `Disposition Using Kepler Data` column is used for classification.

  [Download KOI dataset](https://exoplanetarchive.ipac.caltech.edu/docs/API_kepobjects.html)

This project is licensed under the MIT License.

- **TESS Objects of Interest (TOI)**

---  Dataset containing confirmed exoplanets (KP), planetary candidates (PC), false positives (FP), and ambiguous planetary candidates (APC) from the TESS mission.

  See the `TFOPWG Disposition` column for classification.

## ğŸ™ Acknowledgments  [Download TOI dataset](https://tess.mit.edu/science/toi/)



- **NASA Exoplanet Archive** for datasets- **K2 Planets and Candidates**

- **NASA Space Apps Challenge** for the opportunity  Covers all confirmed exoplanets, planetary candidates, and false positives captured by the K2 mission.

- **scikit-learn & XGBoost teams** for ML libraries  The `Archive Disposition` column provides classification labels.

- **Streamlit** for rapid prototyping  [Download K2 dataset](https://exoplanetarchive.ipac.caltech.edu/docs/K2_objects.html)



---### Research Articles



## ğŸ“§ Contact- **Exoplanet Detection Using Machine Learning**

  Overview of exoplanetary detection methods and a survey of machine learning approaches used in the field up to 2021.

**Team:** Murema Manganyi, Thando, Hlali    [Read Article](https://arxiv.org/abs/2007.14348)

**Challenge:** NASA Space Apps 2025 - Exoplanet Detection

- **Assessment of Ensemble-Based Machine Learning Algorithms for Exoplanet Identification**

---  Discusses preprocessing and ensemble methods that have achieved strong results with the NASA exoplanet datasets.

  [Read Article](https://arxiv.org/abs/2102.06730)

<div align="center">

---

**Built with â¤ï¸ for NASA Space Apps 2025**

## ğŸš€ Tech Stack

*"Finding new worlds, one prediction at a time."* ğŸŒâœ¨

- Python (pandas, scikit-learn, TensorFlow / PyTorch)

</div>- Flask or Streamlit for the web interface

- NASA Exoplanet Archive datasets
- Jupyter Notebook for exploration and model testing

---

## ğŸª Goals

- Build a working ML model that can classify exoplanet data with good accuracy.
- Make the model accessible through a clean, simple web interface.
- Highlight how AI can accelerate exoplanet discovery using real NASA data.

---

## ğŸ Getting Started

Follow these steps to set up the project locally:

1. **Clone this repository**
   ```bash
   git clone https://github.com/murema-v3-exp/nasa-exoplanet-detector.git
   cd nasa-exoplanet-detector
````

2. **Create a Virtual Environment**

    ```bash
    python3 -m venv venv
    # The virtual environment is now created
    ```

3. **Install dependencies**

    ```bash
    # Install core ML and web dependencies
    ./venv/bin/pip install streamlit pandas numpy scikit-learn xgboost matplotlib plotly astropy joblib

    # Optional: Install TensorFlow (for future CNN models)
    ./venv/bin/pip install tensorflow
    ```

4. **Download Datasets** âœ… **Already included!**

    - âœ… KOI (Kepler Objects of Interest): `data/Keppler.csv` (7.4 MB)
    - âœ… K2 Planets and Candidates: `data/K2.csv` (2.6 MB)
    - âœ… TESS Objects of Interest: `data/TESS.csv` (2.0 MB)

5. **Run the Application**

    ```bash
    # Start Streamlit app
    ./venv/bin/python3 -m streamlit run app_enhanced.py --server.port 8501
    ```

    ğŸŒ **Open http://localhost:8501 in your browser**

6.
